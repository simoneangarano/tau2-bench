"""Discrete-time audio native adapter for Deepgram Voice Agent API.

This adapter provides a tick-based interface for Deepgram Voice Agent API, designed
for discrete-time simulation where audio time is the primary clock.

Note: Deepgram Voice Agent is a CASCADED system (STT → LLM → TTS), unlike native
audio models (OpenAI Realtime, Gemini Live, Nova Sonic) that process audio directly.

Key features:
- Tick-based interface via run_tick()
- Audio format conversion (telephony ↔ Deepgram formats)
- Audio capping: max bytes_per_tick of agent audio per tick
- Audio buffering: excess agent audio carries to next tick
- Proportional transcript: text distributed based on audio played
- Interruption handling via UserStartedSpeaking events

Usage:
    adapter = DiscreteTimeDeepgramAdapter(
        tick_duration_ms=1000,
        send_audio_instant=True,
    )
    adapter.connect(system_prompt, tools, vad_config, modality="audio")

    for tick in range(max_ticks):
        result = adapter.run_tick(user_audio_bytes, tick_number=tick)
        # result.get_played_agent_audio() - capped agent audio (telephony format)
        # result.proportional_transcript - text for this tick
        # result.tool_calls - function calls

    adapter.disconnect()
"""

import asyncio
import base64
import json
import threading
import uuid
from typing import Any, Dict, List, Optional, Tuple

from loguru import logger

from tau2.config import TELEPHONY_ULAW_SILENCE
from tau2.data_model.audio import TELEPHONY_AUDIO_FORMAT
from tau2.data_model.message import ToolCall
from tau2.environment.tool import Tool
from tau2.voice.audio_native.adapter import DiscreteTimeAdapter
from tau2.voice.audio_native.deepgram.audio_utils import (
    DEEPGRAM_OUTPUT_BYTES_PER_SECOND,
    TELEPHONY_BYTES_PER_SECOND,
    StreamingDeepgramConverter,
    calculate_deepgram_bytes_per_tick,
    calculate_telephony_bytes_per_tick,
)
from tau2.voice.audio_native.deepgram.events import (
    DeepgramAgentAudioDoneEvent,
    DeepgramAgentStartedSpeakingEvent,
    DeepgramAudioEvent,
    DeepgramConversationTextEvent,
    DeepgramErrorEvent,
    DeepgramFunctionCallRequestEvent,
    DeepgramTimeoutEvent,
    DeepgramUserStartedSpeakingEvent,
)
from tau2.voice.audio_native.deepgram.provider import (
    DeepgramVADConfig,
    DeepgramVoiceAgentProvider,
)
from tau2.voice.audio_native.tick_result import TickResult, UtteranceTranscript


class DiscreteTimeDeepgramAdapter(DiscreteTimeAdapter):
    """Adapter for discrete-time full-duplex simulation with Deepgram Voice Agent API.

    Implements DiscreteTimeAdapter for Deepgram Voice Agent.

    This adapter runs an async event loop in a background thread to communicate
    with the Deepgram Voice Agent API, while exposing a synchronous interface for
    the agent and orchestrator.

    Audio format handling:
    - Input: Receives telephony audio (8kHz μ-law), converts to 16kHz PCM16
    - Output: Receives 16kHz PCM16 from Deepgram, converts to 8kHz μ-law

    Attributes:
        tick_duration_ms: Duration of each tick in milliseconds.
        bytes_per_tick: Audio bytes per tick in telephony format (8kHz μ-law).
        send_audio_instant: If True, send audio in one call per tick.
        provider: Optional provider instance. Created lazily if not provided.
        fast_forward_mode: If True, exit tick early when we have enough audio.
    """

    def __init__(
        self,
        tick_duration_ms: int,
        send_audio_instant: bool = True,
        provider: Optional[DeepgramVoiceAgentProvider] = None,
        fast_forward_mode: bool = False,
        llm_provider: Optional[str] = None,
        llm_model: Optional[str] = None,
        tts_model: Optional[str] = None,
    ):
        """Initialize the discrete-time Deepgram adapter.

        Args:
            tick_duration_ms: Duration of each tick in milliseconds. Must be > 0.
            send_audio_instant: If True, send audio in one call (discrete-time mode).
            provider: Optional provider instance. Created lazily if not provided.
            fast_forward_mode: If True, exit tick early when we have enough audio.
            llm_provider: LLM provider (e.g., "open_ai", "anthropic").
            llm_model: LLM model (e.g., "gpt-4o-mini").
            tts_model: TTS model including voice (e.g., "aura-2-thalia-en").
        """
        if tick_duration_ms <= 0:
            raise ValueError(f"tick_duration_ms must be > 0, got {tick_duration_ms}")

        self.tick_duration_ms = tick_duration_ms
        self.send_audio_instant = send_audio_instant
        self.fast_forward_mode = fast_forward_mode
        self.llm_provider = llm_provider
        self.llm_model = llm_model
        self.tts_model = tts_model

        # We work in telephony format externally (8kHz μ-law)
        self.audio_format = TELEPHONY_AUDIO_FORMAT
        self.bytes_per_tick = calculate_telephony_bytes_per_tick(tick_duration_ms)

        # Deepgram output format (16kHz PCM16) - for internal processing
        self._deepgram_output_bytes_per_tick = calculate_deepgram_bytes_per_tick(
            tick_duration_ms, direction="output"
        )

        # Provider - created lazily if not provided
        self._provider = provider
        self._owns_provider = provider is None

        # Audio format converter (preserves state for streaming)
        self._audio_converter = StreamingDeepgramConverter()

        # Async event loop management
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._thread: Optional[threading.Thread] = None
        self._connected = False

        # Tick state
        self._tick_count = 0
        self._cumulative_user_audio_ms = 0

        # Buffered audio and transcripts
        self._buffered_agent_audio: List[Tuple[bytes, Optional[str]]] = []
        self._utterance_transcripts: dict[str, UtteranceTranscript] = {}
        self._current_item_id: Optional[str] = None
        self._skip_item_id: Optional[str] = None

        # Tool result queue (for sending tool results in next tick)
        self._pending_tool_results: List[Tuple[str, str, str, bool]] = []

        # Track tool call info for Deepgram
        # Maps call_id -> function_name
        self._tool_call_info: Dict[str, str] = {}

    @property
    def provider(self) -> DeepgramVoiceAgentProvider:
        """Get the provider, creating it if needed."""
        if self._provider is None:
            self._provider = DeepgramVoiceAgentProvider(
                llm_provider=self.llm_provider,
                llm_model=self.llm_model,
                tts_model=self.tts_model,
            )
        return self._provider

    @property
    def is_connected(self) -> bool:
        """Check if connected to the API."""
        return self._connected and self._loop is not None

    def connect(
        self,
        system_prompt: str,
        tools: List[Tool],
        vad_config: Any = None,
        modality: str = "audio",
    ) -> None:
        """Connect to the Deepgram Voice Agent API and configure the session.

        Args:
            system_prompt: System prompt for the agent.
            tools: List of tools the agent can use.
            vad_config: VAD configuration. Defaults to automatic VAD.
            modality: "audio" or "text" (Deepgram always uses audio).
        """
        if self._connected:
            logger.warning("Already connected, disconnecting first")
            self.disconnect()

        # Default VAD config
        if vad_config is None:
            vad_config = DeepgramVADConfig()

        # Store config for async initialization
        self._connect_config = {
            "system_prompt": system_prompt,
            "tools": tools,
            "vad_config": vad_config,
            "modality": modality,
        }

        # Start background thread with event loop
        self._start_background_loop()

        # Connect and configure in background loop
        future = asyncio.run_coroutine_threadsafe(
            self._async_connect(**self._connect_config),
            self._loop,
        )

        # Wait for connection with timeout
        try:
            future.result(timeout=30.0)
            self._connected = True
            logger.info(
                f"DiscreteTimeDeepgramAdapter connected to Deepgram Voice Agent API "
                f"(tick={self.tick_duration_ms}ms, bytes_per_tick={self.bytes_per_tick})"
            )
        except Exception as e:
            logger.error(
                f"DiscreteTimeDeepgramAdapter failed to connect to Deepgram Voice Agent API: "
                f"{type(e).__name__}: {e}"
            )
            self._stop_background_loop()
            raise RuntimeError(
                f"Failed to connect to Deepgram Voice Agent API: {e}"
            ) from e

    async def _async_connect(
        self,
        system_prompt: str,
        tools: List[Tool],
        vad_config: DeepgramVADConfig,
        modality: str,
    ) -> None:
        """Async connection and configuration."""
        await self.provider.connect()
        await self.provider.configure_session(
            system_prompt=system_prompt,
            tools=tools,
            vad_config=vad_config,
        )

    def disconnect(self) -> None:
        """Disconnect from the API and clean up resources."""
        if not self._connected:
            return

        # Disconnect in background loop
        if self._loop is not None:
            future = asyncio.run_coroutine_threadsafe(
                self._async_disconnect(),
                self._loop,
            )
            try:
                future.result(timeout=5.0)
            except Exception as e:
                logger.warning(f"Error during disconnect: {e}")

        # Stop background loop
        self._stop_background_loop()
        self._connected = False
        self._tick_count = 0
        self._cumulative_user_audio_ms = 0
        self._buffered_agent_audio.clear()
        self._utterance_transcripts.clear()
        self._tool_call_info.clear()
        self._audio_converter.reset()
        logger.info("DiscreteTimeDeepgramAdapter disconnected")

    async def _async_disconnect(self) -> None:
        """Async disconnection."""
        if self._owns_provider and self._provider is not None:
            await self.provider.disconnect()

    def _start_background_loop(self) -> None:
        """Start the background thread with async event loop."""
        if self._loop is not None:
            return

        def run_loop():
            self._loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._loop)
            self._loop.run_forever()

        self._thread = threading.Thread(target=run_loop, daemon=True)
        self._thread.start()

        # Wait for loop to be ready
        import time

        while self._loop is None:
            time.sleep(0.01)

    def _stop_background_loop(self) -> None:
        """Stop the background thread and event loop."""
        if self._loop is not None:
            self._loop.call_soon_threadsafe(self._loop.stop)
            if self._thread is not None:
                self._thread.join(timeout=2.0)
            self._loop = None
            self._thread = None

    def run_tick(
        self, user_audio: bytes, tick_number: Optional[int] = None
    ) -> TickResult:
        """Run one tick of the simulation.

        Args:
            user_audio: User audio bytes in telephony format (8kHz μ-law).
            tick_number: Optional tick number for logging.

        Returns:
            TickResult with audio in telephony format (8kHz μ-law).
        """
        if not self.is_connected:
            raise RuntimeError(
                "Not connected to Deepgram Voice Agent API. Call connect() first."
            )

        if tick_number is None:
            tick_number = self._tick_count
        self._tick_count = tick_number + 1

        # Run tick in background loop
        future = asyncio.run_coroutine_threadsafe(
            self._async_run_tick(user_audio, tick_number),
            self._loop,
        )

        # Wait for result
        try:
            result = future.result(timeout=self.tick_duration_ms / 1000 + 30.0)
            return result
        except Exception as e:
            logger.error(f"Error in run_tick (tick={tick_number}): {e}")
            raise

    async def _async_run_tick(self, user_audio: bytes, tick_number: int) -> TickResult:
        """Async tick execution."""
        # Send any pending tool results first
        for call_id, name, result_str, request_response in self._pending_tool_results:
            await self.provider.send_tool_result(
                call_id=call_id,
                function_name=name,
                result=result_str,
            )
        self._pending_tool_results.clear()

        # Convert user audio from telephony to Deepgram format
        deepgram_audio = self._audio_converter.convert_input(user_audio)

        # Calculate timing
        tick_start = asyncio.get_running_loop().time()

        # Create tick result
        result = TickResult(
            tick_number=tick_number,
            audio_sent_bytes=len(user_audio),
            audio_sent_duration_ms=(len(user_audio) / TELEPHONY_BYTES_PER_SECOND)
            * 1000,
            user_audio_data=user_audio,
            cumulative_user_audio_at_tick_start_ms=self._cumulative_user_audio_ms,
            bytes_per_tick=self.bytes_per_tick,
            bytes_per_second=TELEPHONY_BYTES_PER_SECOND,
            silence_byte=TELEPHONY_ULAW_SILENCE,
        )

        # Add any buffered agent audio from previous tick
        for chunk_data, item_id in self._buffered_agent_audio:
            result.agent_audio_chunks.append((chunk_data, item_id))
        self._buffered_agent_audio.clear()

        # Carry over skip state from previous tick
        result.skip_item_id = self._skip_item_id

        # Send audio to Deepgram
        if len(deepgram_audio) > 0:
            await self.provider.send_audio(deepgram_audio)

        # Receive events for tick duration
        deepgram_audio_received: List[Tuple[bytes, Optional[str]]] = []

        # Calculate remaining time for this tick
        elapsed_so_far = asyncio.get_running_loop().time() - tick_start
        remaining_duration = max(0.01, (self.tick_duration_ms / 1000) - elapsed_so_far)

        # Receive events for the remaining tick duration
        events = await self.provider.receive_events_for_duration(remaining_duration)

        # Process ALL received events
        for event in events:
            await self._process_event(result, event, deepgram_audio_received)

        # Convert Deepgram audio to telephony format and add to result
        for deepgram_bytes, item_id in deepgram_audio_received:
            telephony_bytes = self._audio_converter.convert_output(deepgram_bytes)
            result.agent_audio_chunks.append((telephony_bytes, item_id))

        # Log audio conversion result
        if deepgram_audio_received:
            total_deepgram = sum(len(d) for d, _ in deepgram_audio_received)
            total_telephony = sum(len(d) for d, _ in result.agent_audio_chunks)
            logger.debug(
                f"Audio conversion: {len(deepgram_audio_received)} chunks, "
                f"{total_deepgram} deepgram bytes -> {total_telephony} telephony bytes"
            )

        # Record simulation timing
        result.tick_sim_duration_ms = result.audio_sent_duration_ms

        # Move excess agent audio to buffer for next tick
        self._buffer_excess_audio(result)

        # Calculate proportional transcript
        result.proportional_transcript = self._get_proportional_transcript(result)

        # Update skip state for next tick
        self._skip_item_id = result.skip_item_id

        # Update cumulative user audio tracking
        self._cumulative_user_audio_ms += int(result.audio_sent_duration_ms)

        logger.info(f"Tick {tick_number} completed:\n{result.summary()}")

        return result

    async def _process_event(
        self,
        result: TickResult,
        event: Any,
        deepgram_audio_received: List[Tuple[bytes, Optional[str]]],
    ) -> None:
        """Process a Deepgram event."""
        result.events.append(event)

        # Handle audio events - binary audio is converted to DeepgramAudioEvent by provider
        if isinstance(event, DeepgramAudioEvent):
            # Decode base64 audio
            audio_data = base64.b64decode(event.audio) if event.audio else b""
            logger.debug(
                f"DeepgramAudioEvent: {len(audio_data)} bytes, "
                f"item_id={self._current_item_id}"
            )
            if audio_data:
                item_id = self._current_item_id or str(uuid.uuid4())[:8]

                # Skip audio from truncated item
                if result.skip_item_id is not None and item_id == result.skip_item_id:
                    estimated_telephony_bytes = int(
                        len(audio_data)
                        * TELEPHONY_BYTES_PER_SECOND
                        / DEEPGRAM_OUTPUT_BYTES_PER_SECOND
                    )
                    result.truncated_audio_bytes += estimated_telephony_bytes
                else:
                    deepgram_audio_received.append((audio_data, item_id))

                    # Track for transcript distribution
                    if item_id not in self._utterance_transcripts:
                        self._utterance_transcripts[item_id] = UtteranceTranscript(
                            item_id=item_id
                        )
                    estimated_telephony_bytes = int(
                        len(audio_data)
                        * TELEPHONY_BYTES_PER_SECOND
                        / DEEPGRAM_OUTPUT_BYTES_PER_SECOND
                    )
                    self._utterance_transcripts[item_id].add_audio(
                        estimated_telephony_bytes
                    )

        elif isinstance(event, DeepgramConversationTextEvent):
            # Track transcripts for proportional distribution
            if event.role == "assistant" and event.content:
                item_id = self._current_item_id or str(uuid.uuid4())[:8]
                if item_id not in self._utterance_transcripts:
                    self._utterance_transcripts[item_id] = UtteranceTranscript(
                        item_id=item_id
                    )
                self._utterance_transcripts[item_id].add_transcript(event.content)
                self._current_item_id = item_id

        elif isinstance(event, DeepgramUserStartedSpeakingEvent):
            logger.debug("User started speaking - interruption detected")
            result.vad_events.append("user_started_speaking")
            # Clear buffered audio
            if self._buffered_agent_audio:
                buffered_bytes = sum(len(c[0]) for c in self._buffered_agent_audio)
                result.truncated_audio_bytes += buffered_bytes
                self._buffered_agent_audio.clear()

            # Mark truncation
            result.was_truncated = True
            result.skip_item_id = self._current_item_id

            # Clear current item_id so next response gets a new one
            # This prevents the skip_item_id from blocking all future audio
            self._current_item_id = None

            # Reset audio converter state after interruption
            self._audio_converter.reset()

        elif isinstance(event, DeepgramAgentStartedSpeakingEvent):
            # New agent utterance
            self._current_item_id = str(uuid.uuid4())[:8]
            logger.debug(f"Agent started speaking (item_id={self._current_item_id})")

        elif isinstance(event, DeepgramAgentAudioDoneEvent):
            logger.debug("Agent audio done")
            # Clear skip state - the previous utterance is complete
            self._skip_item_id = None
            result.skip_item_id = None

        elif isinstance(event, DeepgramFunctionCallRequestEvent):
            # Process function calls
            for func in event.functions:
                # Store function name for when we send the result back
                self._tool_call_info[func.id] = func.name

                # Parse arguments
                try:
                    arguments = json.loads(func.arguments) if func.arguments else {}
                except json.JSONDecodeError:
                    arguments = {}

                tool_call = ToolCall(
                    id=func.id,
                    name=func.name,
                    arguments=arguments,
                )
                result.tool_calls.append(tool_call)
                logger.debug(f"Tool call detected: {func.name}({func.id})")

        elif isinstance(event, DeepgramErrorEvent):
            logger.error(
                f"Deepgram error: {event.error_message} (code={event.error_code})"
            )

        elif isinstance(event, DeepgramTimeoutEvent):
            # Normal timeout, continue
            pass

        else:
            logger.debug(f"Event {type(event).__name__} received")

    def _buffer_excess_audio(self, result: TickResult) -> None:
        """Move agent audio exceeding tick cap to buffer for next tick."""
        # If interrupted, don't buffer - discard excess
        if result.was_truncated:
            total_bytes = 0
            keep_chunks: List[Tuple[bytes, Optional[str]]] = []
            discarded_bytes = 0

            for chunk in result.agent_audio_chunks:
                chunk_data, item_id = chunk
                if total_bytes + len(chunk_data) <= self.bytes_per_tick:
                    keep_chunks.append(chunk)
                    total_bytes += len(chunk_data)
                else:
                    space_left = self.bytes_per_tick - total_bytes
                    if space_left > 0:
                        keep_chunks.append((chunk_data[:space_left], item_id))
                        discarded_bytes += len(chunk_data) - space_left
                    else:
                        discarded_bytes += len(chunk_data)
                    total_bytes = self.bytes_per_tick

            result.agent_audio_chunks = keep_chunks
            result.truncated_audio_bytes += discarded_bytes
            self._buffered_agent_audio = []
            return

        # Normal case: buffer excess for next tick
        total_bytes = 0
        keep_chunks: List[Tuple[bytes, Optional[str]]] = []
        buffer_chunks: List[Tuple[bytes, Optional[str]]] = []

        for chunk in result.agent_audio_chunks:
            chunk_data, item_id = chunk
            if total_bytes + len(chunk_data) <= self.bytes_per_tick:
                keep_chunks.append(chunk)
                total_bytes += len(chunk_data)
            else:
                space_left = self.bytes_per_tick - total_bytes
                if space_left > 0:
                    keep_chunks.append((chunk_data[:space_left], item_id))
                    buffer_chunks.append((chunk_data[space_left:], item_id))
                else:
                    buffer_chunks.append(chunk)
                total_bytes = self.bytes_per_tick

        result.agent_audio_chunks = keep_chunks
        self._buffered_agent_audio = buffer_chunks

    def _get_proportional_transcript(self, result: TickResult) -> str:
        """Get proportional transcript for the audio played this tick."""
        if not result.agent_audio_chunks:
            return ""

        # Group audio bytes by item_id
        audio_by_item: dict[str, int] = {}
        for chunk_data, item_id in result.agent_audio_chunks:
            if item_id:
                audio_by_item[item_id] = audio_by_item.get(item_id, 0) + len(chunk_data)

        # Get proportional transcript for each utterance
        transcript_parts = []
        for item_id, audio_bytes in audio_by_item.items():
            if item_id in self._utterance_transcripts:
                ut = self._utterance_transcripts[item_id]
                text = ut.get_transcript_for_audio(audio_bytes)
                if text:
                    transcript_parts.append(text)

        return " ".join(transcript_parts)

    def send_tool_result(
        self,
        call_id: str,
        result: str,
        request_response: bool = True,
    ) -> None:
        """Queue a tool result to be sent in the next tick.

        Args:
            call_id: The tool call ID.
            result: The tool result as a string.
            request_response: If True, request a response after sending.
        """
        # Look up function name from our tracking dictionary
        name = self._tool_call_info.pop(call_id, "unknown")

        # Queue for sending in next tick
        self._pending_tool_results.append((call_id, name, result, request_response))
        logger.debug(f"Queued tool result for {name}(call_id={call_id})")

    def clear_buffers(self) -> None:
        """Clear all internal audio and transcript buffers."""
        self._buffered_agent_audio.clear()
        self._utterance_transcripts.clear()
        self._pending_tool_results.clear()
        self._tool_call_info.clear()
        self._audio_converter.reset()
        self._skip_item_id = None
