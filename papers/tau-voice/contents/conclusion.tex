\section{Conclusion}
\label{sec:conclusion}

\subsection{Limitations}

\textbf{Language and Speech:} We evaluate English only using TTS rather than recorded speech. Since TTS is more enunciated than real callers, our setup represents a lower bar for voice systems. Accent findings via TTS personas should be interpreted as indicative rather than definitive.

\textbf{Evaluation Scope:} We measure task completion and conversational dynamics, but not agent speech generation quality (tone, naturalness), user satisfaction, or partial task success.

\textbf{Simulator Fidelity:} Our simulator is more patient than real users, with perfect memory and instantaneous tool calls. We decouple from wall-clock time for controllability, but validated this choice by testing with artificial 5-second response delays---observing no adverse effects on agent behavior. In practice, the p95 simulator processing time is $\sim$1.5 seconds, well within conversational tolerance.

\textbf{Transcript Injection:} The simulator bypasses ASR on the agent side by feeding transcripts directly to the user simulator LLM. In our error analysis (Section~\ref{sec:analysis}), annotators found agent speech intelligible in 100\% of the 40 sampled tasks, suggesting this simplification has minimal impact.

\subsection{Future Work}

Future directions include tool call latency, agent speech quality evaluation, non-English languages, and human user studies to validate simulator dynamics. Adding cascaded ASR$\rightarrow$LLM$\rightarrow$TTS baselines (supported by \tauvoice{}'s architecture) would help isolate voice modality effects from architecture choices. 

\subsection{Conclusion}

We introduced \textbf{\tauvoice{}}, extending \tautwobench{} to full-duplex voice with 278 tasks across retail, airline, and telecom domains. Our evaluation reveals a substantial voice-text gap: while GPT-5 (reasoning) achieves 80\%, voice agents reach only 29--42\% under clean conditions and 19--30\% under realistic conditions---a 50--61pp gap. Error analysis attributes 75--90\% of failures to agent behavior rather than simulator artifacts, suggesting the benchmark measures genuine agent limitations. We release \tauvoice{} to support development of voice agents that reliably complete tasks under realistic conditions.
% 