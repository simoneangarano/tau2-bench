@inproceedings{li_cb-whisper_2024,
	address = {Torino, Italia},
	title = {{CB}-{Whisper}: {Contextual} {Biasing} {Whisper} {Using} {Open}-{Vocabulary} {Keyword}-{Spotting}},
	shorttitle = {{CB}-{Whisper}},
	url = {https://aclanthology.org/2024.lrec-main.262/},
	abstract = {End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations and terminologies that are not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (KWS) before the decoder. The KWS module leverages text-to-speech (TTS) techniques and a convolutional neural network (CNN) classifier to match the features between the entities and the utterances. To integrate the recognized entities into the Whipser decoder and avoid hallucinations, we carefully crafted multiple prompts with spoken form hints. Experiments show that the KWS module based on Whisper encoder's features can recognize unseen user-defined keywords effectively. More importantly, the proposed CB-Whisper substantially improves the mixed-error-rate (MER) and entity recall compared to the original Whisper model on three internal datasets and two publicly available datasets including Aishell and ACL datasets that cover English-only, Chinese-only, and code-switching scenarios.},
	urldate = {2026-01-29},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Li, Yuang and Li, Yinglu and Zhang, Min and Su, Chang and Yu, Jiawei and Piao, Mengyao and Qiao, Xiaosong and Ma, Miaomiao and Zhao, Yanqing and Yang, Hao},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {2941--2946},
	file = {Full Text PDF:/Users/soham@sierra.ai/Zotero/storage/ZGK5E7R9/Li et al. - 2024 - CB-Whisper Contextual Biasing Whisper Using Open-Vocabulary Keyword-Spotting.pdf:application/pdf},
}

@misc{gartner_conversational_ai_2024,
	title = {Gartner {Survey} {Reveals} 85\% of {Customer} {Service} {Leaders} {Will} {Explore} or {Pilot} {Customer}-{Facing} {Conversational} {GenAI} in 2025},
	url = {https://www.gartner.com/en/newsroom/press-releases/2024-12-09-gartner-survey-reveals-85-percent-of-customer-service-leaders-will-explore-or-pilot-customer-facing-conversational-genai-in-2025},
	abstract = {In 2025, GenAI adoption will be a top priority for \#CSS leaders, but adoption remains a challenge. \#GartnerCSS},
	language = {en},
	urldate = {2026-01-28},
	author = {{Gartner}},
	month = dec,
	year = {2024},
}

@misc{gartner_agentic_ai_2025,
	title = {Gartner {Predicts} {Agentic} {AI} {Will} {Autonomously} {Resolve} 80\% of {Common} {Customer} {Service} {Issues} {Without} {Human} {Intervention} by 2029},
	url = {https://www.gartner.com/en/newsroom/press-releases/2025-03-05-gartner-predicts-agentic-ai-will-autonomously-resolve-80-percent-of-common-customer-service-issues-without-human-intervention-by-20290},
	abstract = {Meta Description},
	language = {en},
	urldate = {2026-01-28},
	author = {{Gartner}},
	month = mar,
	year = {2025},
}

@misc{moore_ai_voice_2025,
	title = {{AI} {Voice} {Agents}: 2025 {Update}},
	shorttitle = {{AI} {Voice} {Agents}},
	url = {https://a16z.com/ai-voice-agents-2025-update/},
	abstract = {Voice is one of the most powerful unlocks for AI application companies. As models improve, AI voice will become the wedge, not the product.},
	language = {en},
	urldate = {2026-01-28},
	journal = {Andreessen Horowitz},
	author = {Moore, Olivia},
	month = jan,
	year = {2025},
}

@misc{ao_sd-eval_2025,
	title = {{SD}-{Eval}: {A} {Benchmark} {Dataset} for {Spoken} {Dialogue} {Understanding} {Beyond} {Words}},
	shorttitle = {{SD}-{Eval}},
	url = {http://arxiv.org/abs/2406.13340},
	doi = {10.48550/arXiv.2406.13340},
	abstract = {Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information. This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction. Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech. Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses. We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation. SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound. To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a process similar to that of SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. We also conduct a comprehensive evaluation using objective evaluation methods (e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses. Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures. Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics. We open-source SD-Eval at https://github.com/amphionspace/SD-Eval.},
	urldate = {2026-01-22},
	publisher = {arXiv},
	author = {Ao, Junyi and Wang, Yuancheng and Tian, Xiaohai and Chen, Dekun and Zhang, Jun and Lu, Lu and Wang, Yuxuan and Li, Haizhou and Wu, Zhizheng},
	month = jan,
	year = {2025},
	note = {arXiv:2406.13340 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted to NeurIPS 2024},
	file = {Full Text PDF:/Users/soham@sierra.ai/Zotero/storage/8WSL255P/Ao et al. - 2025 - SD-Eval A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words.pdf:application/pdf;Snapshot:/Users/soham@sierra.ai/Zotero/storage/LPQQLQPT/2406.html:text/html},
}

@misc{liu_vocalbench_2026,
	title = {{VocalBench}: {Benchmarking} the {Vocal} {Conversational} {Abilities} for {Speech} {Interaction} {Models}},
	shorttitle = {{VocalBench}},
	url = {http://arxiv.org/abs/2505.15727},
	doi = {10.48550/arXiv.2505.15727},
	abstract = {Speech large language models (SpeechLLMs) have extended human-machine interactions from the text modality to the dynamic speech domain. Spoken dialogues convey diverse information, including semantic concepts, acoustic variations, paralanguage cues, and environmental context. However, existing evaluations of speech interaction models lack instances mimicking real scenarios and predominantly focus on the performance of distinct aspects, lacking a comprehensive comparison of critical capabilities between current routines. To address this gap, we propose VocalBench to assess the speech conversational abilities, comprising around 24k carefully curated instances of both English and Mandarin across four key dimensions - semantic quality, acoustic performance, conversational abilities, and robustness, covering 14 user-oriented characters. Experiments on 27 mainstream models reveal the common challenges for current routes, and highlight the need for new insights into next-generation speech interactive systems.},
	urldate = {2026-01-28},
	publisher = {arXiv},
	author = {Liu, Heyang and Wang, Yuhao and Cheng, Ziyang and Liu, Hongcheng and Li, Yiqi and Hou, Yixuan and Wu, Ronghua and Gu, Qunshan and Wang, Yanfeng and Wang, Yu},
	month = jan,
	year = {2026},
	note = {arXiv:2505.15727 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/soham@sierra.ai/Zotero/storage/S8MLVVV3/Liu et al. - 2026 - VocalBench Benchmarking the Vocal Conversational Abilities for Speech Interaction Models.pdf:application/pdf;Snapshot:/Users/soham@sierra.ai/Zotero/storage/RUPY86RS/2505.html:text/html},
}

@misc{yang_air-bench_2024,
	title = {{AIR}-{Bench}: {Benchmarking} {Large} {Audio}-{Language} {Models} via {Generative} {Comprehension}},
	shorttitle = {{AIR}-{Bench}},
	url = {http://arxiv.org/abs/2402.07729},
	doi = {10.48550/arXiv.2402.07729},
	abstract = {Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench ({\textbackslash}textbf\{A\}udio {\textbackslash}textbf\{I\}nst{\textbackslash}textbf\{R\}uction {\textbackslash}textbf\{Bench\}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: {\textbackslash}textit\{foundation\} and {\textbackslash}textit\{chat\} benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Yang, Qian and Xu, Jin and Liu, Wenrui and Chu, Yunfei and Jiang, Ziyue and Zhou, Xiaohuan and Leng, Yichong and Lv, Yuanjun and Zhao, Zhou and Zhou, Chang and Zhou, Jingren},
	month = jul,
	year = {2024},
	note = {arXiv:2402.07729 [eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/RRPCTZYI/Yang et al. - 2024 - AIR-Bench Benchmarking Large Audio-Language Models via Generative Comprehension.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/HGE5HETX/2402.html:text/html},
}

@misc{ji_wavchat_2024,
	title = {{WavChat}: {A} {Survey} of {Spoken} {Dialogue} {Models}},
	shorttitle = {{WavChat}},
	url = {http://arxiv.org/abs/2411.13577},
	doi = {10.48550/arXiv.2411.13577},
	abstract = {Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. The related material is available at https://github.com/jishengpeng/WavChat.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Ji, Shengpeng and Chen, Yifu and Fang, Minghui and Zuo, Jialong and Lu, Jingyu and Wang, Hanting and Jiang, Ziyue and Zhou, Long and Liu, Shujie and Cheng, Xize and Yang, Xiaoda and Wang, Zehan and Yang, Qian and Li, Jian and Jiang, Yidi and He, Jingzhen and Chu, Yunfei and Xu, Jin and Zhao, Zhou},
	month = nov,
	year = {2024},
	note = {arXiv:2411.13577 [eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Multimedia},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/8BSEA82W/Ji et al. - 2024 - WavChat A Survey of Spoken Dialogue Models.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/VDDVLER6/2411.html:text/html},
}

@misc{wang_audiobench_2025,
	title = {{AudioBench}: {A} {Universal} {Benchmark} for {Audio} {Large} {Language} {Models}},
	shorttitle = {{AudioBench}},
	url = {http://arxiv.org/abs/2406.16020},
	doi = {10.48550/arXiv.2406.16020},
	abstract = {We introduce AudioBench, a universal benchmark designed to evaluate Audio Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among which, 7 are newly proposed datasets. The evaluation targets three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic). Despite recent advancements, there lacks a comprehensive benchmark for AudioLLMs on instruction following capabilities conditioned on audio signals. AudioBench addresses this gap by setting up datasets as well as desired evaluation metrics. Besides, we also evaluated the capabilities of five popular models and found that no single model excels consistently across all tasks. We outline the research outlook for AudioLLMs and anticipate that our open-sourced evaluation toolkit, data, and leaderboard will offer a robust testbed for future model developments.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Wang, Bin and Zou, Xunlong and Lin, Geyu and Sun, Shuo and Liu, Zhuohan and Zhang, Wenyu and Liu, Zhengyuan and Aw, AiTi and Chen, Nancy F.},
	month = may,
	year = {2025},
	note = {arXiv:2406.16020 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/F5R3GHR2/Wang et al. - 2025 - AudioBench A Universal Benchmark for Audio Large Language Models.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/ZZYTH266/2406.html:text/html},
}

@misc{si_spokenwoz_2025,
	title = {{SpokenWOZ}: {A} {Large}-{Scale} {Speech}-{Text} {Benchmark} for {Spoken} {Task}-{Oriented} {Dialogue} {Agents}},
	shorttitle = {{SpokenWOZ}},
	url = {http://arxiv.org/abs/2305.13040},
	doi = {10.48550/arXiv.2305.13040},
	abstract = {Task-oriented dialogue (TOD) models have made significant progress in recent years. However, previous studies primarily focus on datasets written by annotators, which has resulted in a gap between academic research and real-world spoken conversation scenarios. While several small-scale spoken TOD datasets are proposed to address robustness issues such as ASR errors, they ignore the unique challenges in spoken conversation. To tackle the limitations, we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD, containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations. SpokenWOZ further incorporates common spoken characteristics such as word-by-word processing and reasoning in spoken language. Based on these characteristics, we present cross-turn slot and reasoning slot detection as new challenges. We conduct experiments on various baselines, including text-modal models, newly proposed dual-modal models, and LLMs, e.g., ChatGPT. The results show that the current models still have substantial room for improvement in spoken conversation, where the most advanced dialogue state tracker only achieves 25.65\% in joint goal accuracy and the SOTA end-to-end model only correctly completes the user request in 52.1\% of dialogues. The dataset, code, and leaderboard are available: https://spokenwoz.github.io/.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Si, Shuzheng and Ma, Wentao and Gao, Haoyu and Wu, Yuchuan and Lin, Ting-En and Dai, Yinpei and Li, Hangyu and Yan, Rui and Huang, Fei and Li, Yongbin},
	month = jun,
	year = {2025},
	note = {arXiv:2305.13040 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/SFU7K3JK/Si et al. - 2025 - SpokenWOZ A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/UZV7CD5H/2305.html:text/html},
}

@misc{yan_uro-bench_2025,
	title = {{URO}-{Bench}: {Towards} {Comprehensive} {Evaluation} for {End}-to-{End} {Spoken} {Dialogue} {Models}},
	shorttitle = {{URO}-{Bench}},
	url = {http://arxiv.org/abs/2502.17810},
	doi = {10.48550/arXiv.2502.17810},
	abstract = {Recent advances in large language models (LLMs) have driven significant progress in end-to-end spoken dialogue models (SDMs). In contrast to text-based LLMs, the evaluation framework for SDMs should encompass both cognitive dimensions (e.g., logical reasoning, knowledge) and speech-related aspects (e.g., paralinguistic cues, audio quality). However, there is still a lack of comprehensive evaluations for SDMs in speech-to-speech (S2S) scenarios. To address this gap, we propose URO-Bench, an extensive benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers evaluations about multilingualism, multi-round dialogues, and paralinguistics. Our benchmark is divided into two difficulty levels: basic track and pro track, each comprising 20 test sets, evaluating the spoken dialogue model's abilities in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed benchmark reveal that current open-source SDMs perform rather well in daily QA tasks, but lag behind their backbone LLMs in terms of instruction-following ability and also suffer from catastrophic forgetting. Their performance in advanced evaluations of paralinguistic information and audio understanding remains subpar, highlighting the need for further research in this direction. We hope that URO-Bench can facilitate the development of spoken dialogue models by providing a multifaceted evaluation of existing models and helping to track progress in this area.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Yan, Ruiqi and Li, Xiquan and Chen, Wenxi and Niu, Zhikang and Yang, Chen and Ma, Ziyang and Yu, Kai and Chen, Xie},
	month = aug,
	year = {2025},
	note = {arXiv:2502.17810 [cs]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/R959KP53/Yan et al. - 2025 - URO-Bench Towards Comprehensive Evaluation for End-to-End Spoken Dialogue Models.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/7Y6LC7AH/2502.html:text/html},
}

@misc{hou_sova-bench_2025,
	title = {{SOVA}-{Bench}: {Benchmarking} the {Speech} {Conversation} {Ability} for {LLM}-based {Voice} {Assistant}},
	shorttitle = {{SOVA}-{Bench}},
	url = {http://arxiv.org/abs/2506.02457},
	doi = {10.48550/arXiv.2506.02457},
	abstract = {Thanks to the steady progress of large language models (LLMs), speech encoding algorithms and vocoder structure, recent advancements have enabled generating speech response directly from a user instruction. However, benchmarking the generated speech quality has been a neglected but critical issue, considering the shift from the pursuit of semantic accuracy to vivid and spontaneous speech flow. Previous evaluation focused on the speech-understanding ability, lacking a quantification of acoustic quality. In this paper, we propose Speech cOnversational Voice Assistant Benchmark (SOVA-Bench), providing a comprehension comparison of the general knowledge, speech recognition and understanding, along with both semantic and acoustic generative ability between available speech LLMs. To the best of our knowledge, SOVA-Bench is one of the most systematic evaluation frameworks for speech LLMs, inspiring the direction of voice interaction systems.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Hou, Yixuan and Liu, Heyang and Wang, Yuhao and Cheng, Ziyang and Wu, Ronghua and Gu, Qunshan and Wang, Yanfeng and Wang, Yu},
	month = jun,
	year = {2025},
	note = {arXiv:2506.02457 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/NWFTZ788/Hou et al. - 2025 - SOVA-Bench Benchmarking the Speech Conversation Ability for LLM-based Voice Assistant.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/8G85CUD9/2506.html:text/html},
}

@misc{zhang_wildspeech-bench_2025,
	title = {{WildSpeech}-{Bench}: {Benchmarking} {End}-to-{End} {SpeechLLMs} in the {Wild}},
	shorttitle = {{WildSpeech}-{Bench}},
	url = {http://arxiv.org/abs/2506.21875},
	doi = {10.48550/arXiv.2506.21875},
	abstract = {Recent multi-modal Large Language Models (LLMs) such as GPT-4o have demonstrated strong capabilities of direct speech interaction. However, the lack of specialized and comprehensive benchmarks for end-to-end speech LLM evaluation hinders optimizing the user experience of Audio LLMs in real-world applications. Existing evaluation methods often adapt text-based benchmarks, overlooking speech's unique characteristics and challenges, including prosody, homophones, stuttering, and differing user expectations. Here, we introduce the first comprehensive benchmark designed to systematically evaluate end-to-end speechLLMs in practical speech conversations. We systematically curate real-world chat data relevant to spoken scenarios, introduce diversity in speaker attributes and acoustic conditions, and augment the dataset with speech-specific phenomena. We further design a query-aware evaluation method to use customized evaluation checklists and prompts to enhance the accuracy of automatic evaluation. We conduct comprehensive testing and detailed analysis of various mainstream speech models, revealing significant differences in model performance across different speech scenarios. The use of query-aware evaluation further enables a finer-grained assessment under various speech-specific scenarios. Our benchmark can provide valuable insights for speech model development and evaluation.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Zhang, Linhao and Zhang, Jian and Lei, Bokai and Wu, Chuhan and Liu, Aiwei and Jia, Wei and Zhou, Xiao},
	month = sep,
	year = {2025},
	note = {arXiv:2506.21875 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/victorbarres/Zotero/storage/FFH7EMHV/Zhang et al. - 2025 - WildSpeech-Bench Benchmarking End-to-End SpeechLLMs in the Wild.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/LX9YHK8V/2506.html:text/html},
}

@misc{li_televal_2025,
	title = {{TELEVAL}: {A} {Dynamic} {Benchmark} {Designed} for {Spoken} {Language} {Models} in {Chinese} {Interactive} {Scenarios}},
	shorttitle = {{TELEVAL}},
	url = {http://arxiv.org/abs/2507.18061},
	doi = {10.48550/arXiv.2507.18061},
	abstract = {Spoken language models (SLMs) have seen rapid progress in recent years, along with the development of numerous benchmarks for evaluating their performance. However, most existing benchmarks primarily focus on evaluating whether SLMs can perform complex tasks comparable to those tackled by large language models (LLMs), often failing to align with how users naturally interact in real-world conversational scenarios. In this paper, we propose TELEVAL, a dynamic benchmark specifically designed to evaluate SLMs' effectiveness as conversational agents in realistic Chinese interactive settings. TELEVAL defines three evaluation dimensions: Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities. It adopts a dialogue format consistent with real-world usage and evaluates text and audio outputs separately. TELEVAL particularly focuses on the model's ability to extract implicit cues from user speech and respond appropriately without additional instructions. Our experiments demonstrate that despite recent progress, existing SLMs still have considerable room for improvement in natural conversational tasks. We hope that TELEVAL can serve as a user-centered evaluation framework that directly reflects the user experience and contributes to the development of more capable dialogue-oriented SLMs.},
	urldate = {2025-11-05},
	publisher = {arXiv},
	author = {Li, Zehan and Chen, Hongjie and Zhang, Yuxin and Zhou, Jing and Wang, Xuening and Lv, Hang and Du, Mengjie and Song, Yaodong and Lian, Jie and Kang, Jian and Li, Jie and Li, Yongxiang and He, Zhongjiang and Li, Xuelong},
	month = jul,
	year = {2025},
	note = {arXiv:2507.18061 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/3TJF8K7W/Li et al. - 2025 - TELEVAL A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/N8DX2XND/2507.html:text/html},
}

@misc{chen_wavrag_2025,
	title = {{WavRAG}: {Audio}-{Integrated} {Retrieval} {Augmented} {Generation} for {Spoken} {Dialogue} {Models}},
	shorttitle = {{WavRAG}},
	url = {http://arxiv.org/abs/2502.14727},
	doi = {10.48550/arXiv.2502.14727},
	abstract = {Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG's unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality.},
	urldate = {2025-11-06},
	publisher = {arXiv},
	author = {Chen, Yifu and Ji, Shengpeng and Wang, Haoxiao and Wang, Ziqing and Chen, Siyu and He, Jinzheng and Xu, Jin and Zhao, Zhou},
	month = feb,
	year = {2025},
	note = {arXiv:2502.14727 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/G6NGW5XC/Chen et al. - 2025 - WavRAG Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/TJ35NXBN/2502.html:text/html},
}

@misc{arora_landscape_2025,
	title = {On {The} {Landscape} of {Spoken} {Language} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {On {The} {Landscape} of {Spoken} {Language} {Models}},
	url = {http://arxiv.org/abs/2504.08528},
	doi = {10.48550/arXiv.2504.08528},
	abstract = {The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both "pure" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.},
	urldate = {2025-11-06},
	publisher = {arXiv},
	author = {Arora, Siddhant and Chang, Kai-Wei and Chien, Chung-Ming and Peng, Yifan and Wu, Haibin and Adi, Yossi and Dupoux, Emmanuel and Lee, Hung-Yi and Livescu, Karen and Watanabe, Shinji},
	month = apr,
	year = {2025},
	note = {arXiv:2504.08528 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/MGSXLIUY/Arora et al. - 2025 - On The Landscape of Spoken Language Models A Comprehensive Survey.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/ZGGSEDNH/2504.html:text/html},
}

@misc{jiang_s2s-arena_2025,
	title = {{S2S}-{Arena}, {Evaluating} {Speech2Speech} {Protocols} on {Instruction} {Following} with {Paralinguistic} {Information}},
	url = {http://arxiv.org/abs/2503.05085},
	doi = {10.48550/arXiv.2503.05085},
	abstract = {The rapid development of large language models (LLMs) has brought significant attention to speech models, particularly recent progress in speech2speech protocols supporting speech input and output. However, the existing benchmarks adopt automatic text-based evaluators for evaluating the instruction following ability of these models lack consideration for paralinguistic information in both speech understanding and generation. To address these issues, we introduce S2S-Arena, a novel arena-style S2S benchmark that evaluates instruction-following capabilities with paralinguistic information in both speech-in and speech-out across real-world tasks. We design 154 samples that fused TTS and live recordings in four domains with 21 tasks and manually evaluate existing popular speech models in an arena-style manner. The experimental results show that: (1) in addition to the superior performance of GPT-4o, the speech model of cascaded ASR, LLM, and TTS outperforms the jointly trained model after text-speech alignment in speech2speech protocols; (2) considering paralinguistic information, the knowledgeability of the speech model mainly depends on the LLM backbone, and the multilingual support of that is limited by the speech module; (3) excellent speech models can already understand the paralinguistic information in speech input, but generating appropriate audio with paralinguistic information is still a challenge.},
	urldate = {2025-11-06},
	publisher = {arXiv},
	author = {Jiang, Feng and Lin, Zhiyu and Bu, Fan and Du, Yuhao and Wang, Benyou and Li, Haizhou},
	month = mar,
	year = {2025},
	note = {arXiv:2503.05085 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/UPZIIPAI/Jiang et al. - 2025 - S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following with Paralinguistic Informati.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/JIJBI9E9/2503.html:text/html},
}

@misc{lin_full-duplex-bench_2025,
	title = {Full-{Duplex}-{Bench}: {A} {Benchmark} to {Evaluate} {Full}-duplex {Spoken} {Dialogue} {Models} on {Turn}-taking {Capabilities}},
	shorttitle = {Full-{Duplex}-{Bench}},
	url = {http://arxiv.org/abs/2503.04721},
	doi = {10.48550/arXiv.2503.04721},
	abstract = {Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.},
	urldate = {2025-11-06},
	publisher = {arXiv},
	author = {Lin, Guan-Ting and Lian, Jiachen and Li, Tingle and Wang, Qirui and Anumanchipalli, Gopala and Liu, Alexander H. and Lee, Hung-yi},
	month = aug,
	year = {2025},
	note = {arXiv:2503.04721 [cs]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/HRKH5EYR/Lin et al. - 2025 - Full-Duplex-Bench A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabil.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/6T3AYCXX/2503.html:text/html},
}

@misc{chen_voicebench_2024,
	title = {{VoiceBench}: {Benchmarking} {LLM}-{Based} {Voice} {Assistants}},
	shorttitle = {{VoiceBench}},
	url = {http://arxiv.org/abs/2410.17196},
	doi = {10.48550/arXiv.2410.17196},
	abstract = {Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.},
	urldate = {2025-11-06},
	publisher = {arXiv},
	author = {Chen, Yiming and Yue, Xianghu and Zhang, Chen and Gao, Xiaoxue and Tan, Robby T. and Li, Haizhou},
	month = dec,
	year = {2024},
	note = {arXiv:2410.17196 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/IWSNTZJA/Chen et al. - 2024 - VoiceBench Benchmarking LLM-Based Voice Assistants.pdf:application/pdf},
}

@misc{lin_full-duplex-bench-v2_2025,
	title = {Full-{Duplex}-{Bench}-v2: {A} {Multi}-{Turn} {Evaluation} {Framework} for {Duplex} {Dialogue} {Systems} with an {Automated} {Examiner}},
	shorttitle = {Full-{Duplex}-{Bench}-v2},
	url = {http://arxiv.org/abs/2510.07838},
	doi = {10.48550/arXiv.2510.07838},
	abstract = {While full-duplex speech agents enable natural, low-latency interaction by speaking and listening simultaneously, their consistency and task performance in multi-turn settings remain underexplored. We introduce Full-Duplex-Bench-v2 (FDB-v2), a streaming framework that integrates with an automated examiner that enforces staged goals under two pacing setups (Fast vs. Slow). FDB-v2 covers four task families: daily, correction, entity tracking, and safety. We report turn-taking fluency, multi-turn instruction following, and task-specific competence. The framework is extensible, supporting both commercial APIs and open source models. When we test full-duplex systems with FDB-v2, they often get confused when people talk at the same time, struggle to handle corrections smoothly, and sometimes lose track of who or what is being talked about. Through an open-sourced, standardized streaming protocol and a task set, FDB-v2 makes it easy to extend to new task families, allowing the community to tailor and accelerate evaluation of multi-turn full-duplex systems.},
	urldate = {2025-11-06},
	publisher = {arXiv},
	author = {Lin, Guan-Ting and Kuan, Shih-Yun Shan and Shi, Jiatong and Chang, Kai-Wei and Arora, Siddhant and Watanabe, Shinji and Lee, Hung-yi},
	month = oct,
	year = {2025},
	note = {arXiv:2510.07838 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/LDJHLSXK/Lin et al. - 2025 - Full-Duplex-Bench-v2 A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automate.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/UHKNM8RN/2510.html:text/html},
}

@misc{yang_paras2s_2025,
	title = {{ParaS2S}: {Benchmarking} and {Aligning} {Spoken} {Language} {Models} for {Paralinguistic}-aware {Speech}-to-{Speech} {Interaction}},
	shorttitle = {{ParaS2S}},
	url = {http://arxiv.org/abs/2511.08723},
	doi = {10.48550/arXiv.2511.08723},
	abstract = {Speech-to-Speech (S2S) models have shown promising dialogue capabilities, but their ability to handle paralinguistic cues--such as emotion, tone, and speaker attributes--and to respond appropriately in both content and style remains underexplored. Progress is further hindered by the scarcity of high-quality and expressive demonstrations. To address this, we introduce a novel reinforcement learning (RL) framework for paralinguistic-aware S2S, ParaS2S, which evaluates and optimizes both content and speaking style directly at the waveform level. We first construct ParaS2SBench, a benchmark comprehensively evaluates S2S models' output for content and style appropriateness from diverse and challenging input queries. It scores the fitness of input-output pairs and aligns well with human judgments, serving as an automatic judge for model outputs. With this scalable scoring feedback, we enable the model to explore and learn from diverse unlabeled speech via Group Relative Policy Optimization (GRPO). Experiments show that existing S2S models fail to respond appropriately to paralinguistic attributes, performing no better than pipeline-based baselines. Our RL approach achieves a 11\% relative improvement in response content and style's appropriateness on ParaS2SBench over supervised fine-tuning (SFT), surpassing all prior models while requiring substantially fewer warm-up annotations than pure SFT.},
	urldate = {2026-01-06},
	publisher = {arXiv},
	author = {Yang, Shu-wen and Tu, Ming and Liu, Andy T. and Qu, Xinghua and Lee, Hung-yi and Lu, Lu and Wang, Yuxuan and Wu, Yonghui},
	month = nov,
	year = {2025},
	note = {arXiv:2511.08723 [eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/2VMCB9NS/Yang et al. - 2025 - ParaS2S Benchmarking and Aligning Spoken Language Models for Paralinguistic-aware Speech-to-Speech.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/DG8EAL6C/2511.html:text/html},
}

@misc{gosai_audio_2025,
	title = {Audio {MultiChallenge}: {A} {Multi}-{Turn} {Evaluation} of {Spoken} {Dialogue} {Systems} on {Natural} {Human} {Interaction}},
	shorttitle = {Audio {MultiChallenge}},
	url = {http://arxiv.org/abs/2512.14865},
	doi = {10.48550/arXiv.2512.14865},
	abstract = {End-to-end (E2E) spoken dialogue systems are increasingly replacing cascaded pipelines for voice-based human-AI interaction, processing raw audio directly without intermediate transcription. Existing benchmarks primarily evaluate these models on synthetic speech and single-turn tasks, leaving realistic multi-turn conversational ability underexplored. We introduce Audio MultiChallenge, an open-source benchmark to evaluate E2E spoken dialogue systems under natural multi-turn interaction patterns. Building on the text-based MultiChallenge framework, which evaluates Inference Memory, Instruction Retention, and Self Coherence, we introduce a new axis Voice Editing that tests robustness to mid-utterance speech repairs and backtracking. We further augment each axis to the audio modality, such as introducing Audio-Cue challenges for Inference Memory that require recalling ambient sounds and paralinguistic signals beyond semantic content. We curate 452 conversations from 47 speakers with 1,712 instance-specific rubrics through a hybrid audio-native agentic and human-in-the-loop pipeline that exposes model failures at scale while preserving natural disfluencies found in unscripted human speech. Our evaluation of proprietary and open-source models reveals that even frontier models struggle on our benchmark, with Gemini 3 Pro Preview (Thinking), our highest-performing model achieving a 54.65\% pass rate. Error analysis shows that models fail most often on our new axes and that Self Coherence degrades with longer audio context. These failures reflect difficulty of tracking edits, audio cues, and long-range context in natural spoken dialogue. Audio MultiChallenge provides a reproducible testbed to quantify them and drive improvements in audio-native multi-turn interaction capability.},
	urldate = {2026-01-07},
	publisher = {arXiv},
	author = {Gosai, Advait and Vuong, Tyler and Tyagi, Utkarsh and Li, Steven and You, Wenjia and Bavare, Miheer and Uçar, Arda and Fang, Zhongwang and Jang, Brian and Liu, Bing and He, Yunzhong},
	month = dec,
	year = {2025},
	note = {arXiv:2512.14865 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/P96YU7KX/Gosai et al. - 2025 - Audio MultiChallenge A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interactio.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/GVKIDAQH/2512.html:text/html},
}

@misc{elevenlabs_eleven_2025,
	title = {Eleven v3: {Most} {Expressive} {AI} {TTS} {Model} {Launched}},
	shorttitle = {Eleven v3},
	url = {https://elevenlabs.io/blog/eleven-v3},
	abstract = {Eleven v3 (alpha) introduces advanced audio tags, dialogue mode, and 70+ languages for nuanced, emotionally rich AI-generated speech.},
	language = {en},
	urldate = {2026-01-13},
	journal = {ElevenLabs},
	author = {ElevenLabs},
	month = dec,
	year = {2025},
	file = {Snapshot:/Users/victorbarres/Zotero/storage/5DTZXTJ4/eleven-v3.html:text/html},
}

@misc{openai_introducing_2025,
	title = {Introducing gpt-realtime and {Realtime} {API} updates for production voice agents},
	url = {https://openai.com/index/introducing-gpt-realtime/},
	abstract = {We’re releasing a more advanced speech-to-speech model and new API capabilities including MCP server support, image input, and SIP phone calling support.},
	language = {en-US},
	urldate = {2026-01-13},
	author = {OpenAI},
	month = aug,
	year = {2025},
	file = {Snapshot:/Users/victorbarres/Zotero/storage/E6CT6JJT/introducing-gpt-realtime.html:text/html},
}

@misc{vertex_ai_gemini_2025,
	title = {Gemini {Live} {API} available on {Vertex} {AI}},
	url = {https://cloud.google.com/blog/products/ai-machine-learning/gemini-live-api-available-on-vertex-ai},
	abstract = {Google Cloud customers can now deploy mission-critical, low-latency voice and video agents with the stability, performance, and governance required for your most demanding workflows.},
	language = {en},
	urldate = {2026-01-13},
	journal = {Google Cloud Blog},
	author = {Vertex AI, Google Cloud},
	month = dec,
	year = {2025},
	file = {Snapshot:/Users/victorbarres/Zotero/storage/IKLDS8UK/gemini-live-api-available-on-vertex-ai.html:text/html},
}

@misc{xai_grok_voice_2025,
	title = {Grok {Voice} {Agent} {API}},
	url = {https://x.ai/news/grok-voice-agent-api},
	abstract = {Bringing the power of Grok Voice to all developers.},
	language = {en},
	urldate = {2026-01-28},
	author = {{xAI}},
	month = dec,
	year = {2025},
}

@misc{barres_2-bench_2025,
	title = {$\tau^2$-{Bench}: {Evaluating} {Conversational} {Agents} in a {Dual}-{Control} {Environment}},
	shorttitle = {$\tau^2$-{Bench}},
	url = {http://arxiv.org/abs/2506.07982},
	doi = {10.48550/arXiv.2506.07982},
	abstract = {Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce $\tau^2$-bench, with four key contributions: 1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication, 2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity, 3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity, 4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination. In particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, $\tau^2$-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.},
	urldate = {2026-01-13},
	publisher = {arXiv},
	author = {Barres, Victor and Dong, Honghua and Ray, Soham and Si, Xujie and Narasimhan, Karthik},
	month = jun,
	year = {2025},
	note = {arXiv:2506.07982 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/YQSDEREQ/Barres et al. - 2025 - tau2-Bench Evaluating Conversational Agents in a Dual-Control Environment.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/MVB375NV/2506.html:text/html},
}

@misc{yao_-bench_2024,
	title = {$\tau$-bench: {A} {Benchmark} for {Tool}-{Agent}-{User} {Interaction} in {Real}-{World} {Domains}},
	shorttitle = {$\tau$-bench},
	url = {http://arxiv.org/abs/2406.12045},
	doi = {10.48550/arXiv.2406.12045},
	abstract = {Existing benchmarks do not test language agents on their interaction with human users or ability to follow domain-specific rules, both of which are vital for deploying them in real world applications. We propose $\tau$-bench, a benchmark emulating dynamic conversations between a user (simulated by language models) and a language agent provided with domain-specific API tools and policy guidelines. We employ an efficient and faithful evaluation process that compares the database state at the end of a conversation with the annotated goal state. We also propose a new metric (pass{\textasciicircum}k) to evaluate the reliability of agent behavior over multiple trials. Our experiments show that even state-of-the-art function calling agents (like gpt-4o) succeed on {\textless}50\% of the tasks, and are quite inconsistent (pass{\textasciicircum}8 {\textless}25\% in retail). Our findings point to the need for methods that can improve the ability of agents to act consistently and follow rules reliably.},
	urldate = {2026-01-13},
	publisher = {arXiv},
	author = {Yao, Shunyu and Shinn, Noah and Razavi, Pedram and Narasimhan, Karthik},
	month = jun,
	year = {2024},
	note = {arXiv:2406.12045 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/victorbarres/Zotero/storage/S8439ZUN/Yao et al. - 2024 - $\tau$-bench A Benchmark for Tool-Agent-User Interaction in Real-World Domains.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/IM7RV3DF/2406.html:text/html},
}

@inproceedings{raux_finite-state_2009,
	address = {Boulder, Colorado},
	title = {A {Finite}-{State} {Turn}-{Taking} {Model} for {Spoken} {Dialog} {Systems}},
	url = {https://aclanthology.org/N09-1071/},
	urldate = {2026-01-15},
	booktitle = {Proceedings of {Human} {Language} {Technologies}: {The} 2009 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Raux, Antoine and Eskenazi, Maxine},
	editor = {Ostendorf, Mari and Collins, Michael and Narayanan, Shri and Oard, Douglas W. and Vanderwende, Lucy},
	month = jun,
	year = {2009},
	pages = {629--637},
	file = {Full Text PDF:/Users/victorbarres/Zotero/storage/AQRJPDSV/Raux and Eskenazi - 2009 - A Finite-State Turn-Taking Model for Spoken Dialog Systems.pdf:application/pdf},
}

@inproceedings{cathcart_model_2003,
	address = {Budapest, Hungary},
	title = {A model of back-channel acknowledgements in spoken dialogue},
	url = {https://aclanthology.org/E03-1069/},
	urldate = {2026-01-15},
	booktitle = {10th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Cathcart, Nicola and Carletta, Jean and Klein, Ewan},
	editor = {Copestake, Ann and Hajič, Jan},
	month = apr,
	year = {2003},
	file = {Full Text PDF:/Users/victorbarres/Zotero/storage/95NCN7TA/Cathcart et al. - 2003 - A model of back-channel acknowledgements in spoken dialogue.pdf:application/pdf},
}

@article{blomsma_backchannel_2024,
	title = {Backchannel behavior is idiosyncratic},
	volume = {16},
	issn = {1866-9808, 1866-9859},
	url = {https://www.cambridge.org/core/journals/language-and-cognition/article/backchannel-behavior-is-idiosyncratic/F75D0AEEAF258399166A58E3DDCC7D7E},
	doi = {10.1017/langcog.2024.1},
	abstract = {In spoken conversations, speakers and their addressees constantly seek and provide different forms of audiovisual feedback, also known as backchannels, which include nodding, vocalizations and facial expressions. It has previously been shown that addressees backchannel at specific points during an interaction, namely after a speaker provided a cue to elicit feedback from the addressee. However, addressees may differ in the frequency and type of feedback that they provide, and likewise, speakers may vary the type of cues they generate to signal the backchannel opportunity points (BOPs). Research on the extent to which backchanneling is idiosyncratic is scant. In this article, we quantify and analyze the variability in feedback behavior of 14 addressees who all interacted with the same speaker stimulus. We conducted this research by means of a previously developed experimental paradigm that generates spontaneous interactions in a controlled manner. Our results show that (1) backchanneling behavior varies between listeners (some addressees are more active than others) and (2) backchanneling behavior varies between BOPs (some points trigger more responses than others). We discuss the relevance of these results for models of human–human and human–machine interactions.},
	language = {en},
	number = {4},
	urldate = {2026-01-15},
	journal = {Language and Cognition},
	author = {Blomsma, Peter and Vaitonyté, Julija and Skantze, Gabriel and Swerts, Marc},
	month = dec,
	year = {2024},
	keywords = {backchannels, consensus sampling, head nod, listener feedback, multimodal, O-Cam paradigm},
	pages = {1158--1181},
	file = {Full Text PDF:/Users/victorbarres/Zotero/storage/C9TKEBIU/Blomsma et al. - 2024 - Backchannel behavior is idiosyncratic.pdf:application/pdf},
}

@misc{arora_talking_2025,
	title = {Talking {Turns}: {Benchmarking} {Audio} {Foundation} {Models} on {Turn}-{Taking} {Dynamics}},
	shorttitle = {Talking {Turns}},
	url = {http://arxiv.org/abs/2503.01174},
	doi = {10.48550/arXiv.2503.01174},
	abstract = {The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Arora, Siddhant and Lu, Zhiyun and Chiu, Chung-Cheng and Pang, Ruoming and Watanabe, Shinji},
	month = mar,
	year = {2025},
	note = {arXiv:2503.01174 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Full Text PDF:/Users/victorbarres/Zotero/storage/DIGDNW2G/Arora et al. - 2025 - Talking Turns Benchmarking Audio Foundation Models on Turn-Taking Dynamics.pdf:application/pdf;Snapshot:/Users/victorbarres/Zotero/storage/USUNLPHT/2503.html:text/html},
}
